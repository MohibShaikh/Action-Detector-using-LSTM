{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Action Recognition",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohibShaikh/Action-Detector-using-LSTM/blob/main/Action_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *1. Installing Dependencies*"
      ],
      "metadata": {
        "id": "5VB7pwRRz8jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install opencv-python\n",
        "!pip install mediapipe\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-14T21:19:53.117314Z",
          "iopub.execute_input": "2024-04-14T21:19:53.118581Z",
          "iopub.status.idle": "2024-04-14T21:21:14.338178Z",
          "shell.execute_reply.started": "2024-04-14T21:19:53.118527Z",
          "shell.execute_reply": "2024-04-14T21:21:14.336613Z"
        },
        "trusted": true,
        "id": "dD45Lp9Jz8js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import mediapipe as mp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T21:21:14.340693Z",
          "iopub.execute_input": "2024-04-14T21:21:14.341068Z",
          "iopub.status.idle": "2024-04-14T21:21:29.955227Z",
          "shell.execute_reply.started": "2024-04-14T21:21:14.341035Z",
          "shell.execute_reply": "2024-04-14T21:21:29.953972Z"
        },
        "trusted": true,
        "id": "lHiRdS7-z8jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *2. Keypoints using MP Holistic*"
      ],
      "metadata": {
        "id": "FdKgvCKlz8ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic       # Holistic Model\n",
        "mp_drawing = mp.solutions.drawing_utils   # Drawing Utilities"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T21:21:29.956976Z",
          "iopub.execute_input": "2024-04-14T21:21:29.958091Z",
          "iopub.status.idle": "2024-04-14T21:21:29.96329Z",
          "shell.execute_reply.started": "2024-04-14T21:21:29.958046Z",
          "shell.execute_reply": "2024-04-14T21:21:29.962219Z"
        },
        "trusted": true,
        "id": "Ng1KFgptz8ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "    '''Draws face, pose, left hand, and right hand connections.'''\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp.holistic.FACEMESH_CONTOURS)\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp.holistic.POSE_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp.holistic.HAND_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp.holistic.HAND_CONNECTIONS)"
      ],
      "metadata": {
        "id": "etHFv_tJz8ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def styled_landmarks(image, results):\n",
        "    '''Draws face, pose, left hand, and right hand connections.'''\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp.holistic.FACE_CONNECTIONS\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1,\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp.holistic.POSE_CONNECTIONS\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10),thickness=1,circle_radius=4,\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121),thickness=1,circle_radius=2)\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp.holistic.HAND_CONNECTIONS\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76),thickness=1,circle_radius=4,\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250),thickness=1,circle_radius=2)\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp.holistic.HAND_CONNECTIONS\n",
        "                             mp_drawing.DrawingSpec(color=(245,192,30),thickness=1,circle_radius=4,\n",
        "                             mp_drawing.DrawingSpec(color=(234,90,200),thickness=1,circle_radius=2)"
      ],
      "metadata": {
        "id": "zXzyWd3gz8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    '''Takes an image and a mediapipe model, processes it, makes a prediction, returns a result image, and a prediction'''\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)   #  Converting to BGR on which the model prediction happens\n",
        "    image.flag.writeable = False                     # Image not writeable anymore (doing this saves memory)\n",
        "    results = model.process(image)                   # Make Prediction\n",
        "    image.flag.writeable = True                      # Image now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)   # Converting back to originall color\n",
        "    return image, results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T21:25:41.642493Z",
          "iopub.execute_input": "2024-04-14T21:25:41.643022Z",
          "iopub.status.idle": "2024-04-14T21:25:41.650246Z",
          "shell.execute_reply.started": "2024-04-14T21:25:41.642981Z",
          "shell.execute_reply": "2024-04-14T21:25:41.649298Z"
        },
        "trusted": true,
        "id": "izd8iYLMz8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(2)\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened:\n",
        "\n",
        "        # Reads the image\n",
        "        ret,frame = cap.read()\n",
        "\n",
        "        # Makes a predition\n",
        "        image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "        # Draw Landmarks\n",
        "        styled_landmarks(image, results)\n",
        "\n",
        "        # Displays the output image\n",
        "        cv2.imshow('OpenCV Feed',image)\n",
        "\n",
        "        if cv2.waitkey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "trusted": true,
        "id": "uOoerxRlz8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Value Points Extraction"
      ],
      "metadata": {
        "id": "-yox0AVqz8jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.pose_landmarks"
      ],
      "metadata": {
        "id": "GOUgIAIDz8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poses=[]\n",
        "for res in results.pose_landmarks.landmark:\n",
        "    test = np.array(res.x,res.y,res.z,res.visibility)\n",
        "    poses.append(test)"
      ],
      "metadata": {
        "id": "4zJoaj4Gz8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(results):\n",
        "    poses = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if face_landmarks else np.zeroes(33*4)\n",
        "    lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if left_hand_landmarks else np.zeroes(21*3)\n",
        "    rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if right_hand_landmarks else np.zeroes(21*3)\n",
        "    face =np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if face_landmarks else np.zeroes(1404)\n",
        "    return concatenate([poses,lh,rh,face])"
      ],
      "metadata": {
        "id": "_aZ_X_vVz8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *4. Setup Folders for Collection*"
      ],
      "metadata": {
        "id": "HkkF43VUz8jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = os.path.join('MP_DATA')\n",
        "\n",
        "actions = ['hello','iloveyou','thanks']\n",
        "\n",
        "no_squences = 30\n",
        "\n",
        "sequence_length = 30"
      ],
      "metadata": {
        "id": "R1G8lf0az8jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for action in actions:\n",
        "    for i in range(sequence_length):\n",
        "        try:\n",
        "            os.make_dirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "5oZjZxVFz8jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(2)\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "    for action in actions:\n",
        "        for sequence in range(no_sequences):\n",
        "            for frame_num in range(sequence_length):\n",
        "\n",
        "                # Reads the image\n",
        "                ret,frame = cap.read()\n",
        "\n",
        "                # Makes a predition\n",
        "                image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "                # Draw Landmarks\n",
        "                styled_landmarks(image, results)\n",
        "\n",
        "                # Applying wait logic\n",
        "                if frame_num == 0:\n",
        "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
        "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                    # Show to screen\n",
        "                    cv2.imshow('OpenCV Feed', image)\n",
        "                    cv2.waitKey(2000)\n",
        "                else:\n",
        "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                    # Show to screen\n",
        "                    cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "\n",
        "                # Displays the output image\n",
        "                cv2.imshow('OpenCV Feed',image)\n",
        "\n",
        "        if cv2.waitkey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Pmj4fsoFz8jw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}