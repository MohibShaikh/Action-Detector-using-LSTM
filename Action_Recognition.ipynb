{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Action Recognition",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohibShaikh/Action-Detector-using-LSTM/blob/main/Action_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *1. Installing Dependencies*"
      ],
      "metadata": {
        "id": "w108BdYEkwAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install opencv-python\n",
        "!pip install mediapipe\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-14T21:19:53.117314Z",
          "iopub.execute_input": "2024-04-14T21:19:53.118581Z",
          "iopub.status.idle": "2024-04-14T21:21:14.338178Z",
          "shell.execute_reply.started": "2024-04-14T21:19:53.118527Z",
          "shell.execute_reply": "2024-04-14T21:21:14.336613Z"
        },
        "trusted": true,
        "id": "FyjOqFIdkwAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import mediapipe as mp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T21:21:14.340693Z",
          "iopub.execute_input": "2024-04-14T21:21:14.341068Z",
          "iopub.status.idle": "2024-04-14T21:21:29.955227Z",
          "shell.execute_reply.started": "2024-04-14T21:21:14.341035Z",
          "shell.execute_reply": "2024-04-14T21:21:29.953972Z"
        },
        "trusted": true,
        "id": "4QUIqnzvkwAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *2. Keypoints using MP Holistic*"
      ],
      "metadata": {
        "id": "4RokBWRbkwAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic       # Holistic Model\n",
        "mp_drawing = mp.solutions.drawing_utils   # Drawing Utilities"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T21:21:29.956976Z",
          "iopub.execute_input": "2024-04-14T21:21:29.958091Z",
          "iopub.status.idle": "2024-04-14T21:21:29.96329Z",
          "shell.execute_reply.started": "2024-04-14T21:21:29.958046Z",
          "shell.execute_reply": "2024-04-14T21:21:29.962219Z"
        },
        "trusted": true,
        "id": "a724f_JxkwAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "    '''Draws face, pose, left hand, and right hand connections.'''\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp.holistic.FACE_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp.holistic.POSE_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp.holistic.HAND_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp.holistic.HAND_CONNECTIONS)"
      ],
      "metadata": {
        "id": "yKBjX3ybkwAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    '''Takes an image and a mediapipe model, processes it, makes a prediction, returns a result image, and a prediction'''\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)   #  Converting to BGR on which the model prediction happens\n",
        "    image.flag.writeable = False                     # Image not writeable anymore (doing this saves memory)\n",
        "    results = model.process(image)                   # Make Prediction\n",
        "    image.flag.writeable = True                      # Image now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)   # Converting back to originall color\n",
        "    return image, results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T21:25:41.642493Z",
          "iopub.execute_input": "2024-04-14T21:25:41.643022Z",
          "iopub.status.idle": "2024-04-14T21:25:41.650246Z",
          "shell.execute_reply.started": "2024-04-14T21:25:41.642981Z",
          "shell.execute_reply": "2024-04-14T21:25:41.649298Z"
        },
        "trusted": true,
        "id": "mJ4VIXoBkwAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(2)\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened:\n",
        "\n",
        "        # Reads the image\n",
        "        ret,frame = cap.read()\n",
        "\n",
        "        # Makes a predition\n",
        "        image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "        # Draw Landmarks\n",
        "        draw_landmarks(image, results)\n",
        "\n",
        "        # Displays the output image\n",
        "        cv2.imshow('OpenCV Feed',image)\n",
        "\n",
        "        if cv2.waitkey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kmB7ZUy9kwAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47hyL22ZkwAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}